{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('tp-walmart').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2 : Load the Walmart Stock CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"walmart_stock.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3 : What are the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 : What does the Schema look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      " |-- Adj Close: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5 : Create a new DataFrame with a column called HV_Ration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+---------+------------------+--------+------------------+--------------------+\n",
      "|      Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|            HV_Ratio|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+--------------------+\n",
      "|2012-01-03|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|4.819714653321546E-6|\n",
      "|2012-01-04|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|6.290848613094555E-6|\n",
      "|2012-01-05|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|4.669412994783916E-6|\n",
      "+----------+------------------+---------+---------+------------------+--------+------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "dt = df.withColumn('HV_Ratio', F.col('High')/F.col('Volume'))\n",
    "dt.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6 : What day had the Peak High in Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation dataframe en table\n",
    "dt.createOrReplaceTempView(\"dtSQL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date='2015-01-13')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# en SQL\n",
    "spark.sql(\"\"\"SELECT Date FROM dtSQL ORDER BY High Desc\"\"\").first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Date='2015-01-13')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# en DSL\n",
    "dt.select(F.col('Date')).orderBy(F.col('High').desc()).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7 : What is the mean of the Close column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|          Moyenne|\n",
      "+-----------------+\n",
      "|72.38844998012726|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# en SQL\n",
    "spark.sql(\"\"\"SELECT MEAN(Close) AS Moyenne FROM dtSQL\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|          Moyenne|\n",
      "+-----------------+\n",
      "|72.38844998012726|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# en DSL\n",
    "dt.agg(F.mean('Close').alias('Moyenne')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8 : What is the max and the min of the Volume column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|Maximum| Minimum|\n",
      "+-------+--------+\n",
      "|9994400|10010500|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# en SQL\n",
    "spark.sql(\"\"\"SELECT Max(Volume) AS Maximum, Min(Volume) AS Minimum FROM dtSQL\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|Maximum| Minimum|\n",
      "+-------+--------+\n",
      "|9994400|10010500|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# en DSL\n",
    "dt.agg(F.max('Volume').alias('Maximum'), F.min('Volume').alias('Minimum')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q9 : How many days was the Close lower than 60 dollars "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|Nombre|\n",
      "+------+\n",
      "|    81|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# en SQL\n",
    "spark.sql(\"\"\"SELECT Count(Date) as Nombre FROM dtSQL WHERE Close<'60'\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# en DSL\n",
    "dt.filter(F.col(\"Close\")<'60').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q10 : What percentage of the time was the High greater than 80 dollars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|Percentage|\n",
      "+----------+\n",
      "|      9.14|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# en SQL\n",
    "spark.sql(\"\"\"SELECT Round(\n",
    "          (SELECT Count(*) FROM dtSQL WHERE High>=80)/(count(*))*100, 2) AS Percentage FROM dtSQL\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Pourcentage|\n",
      "+-----------+\n",
      "|       9.14|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# en DSL\n",
    "tmp = dt.filter(F.col('High')>'80').agg(F.count(\"*\").alias(\"Comptage\")).collect()[0][0]\n",
    "dt.agg(F.round((tmp/F.count(\"*\")*100),2).alias(\"Pourcentage\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q11 : What is the max High per year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|  Maximum|Annee|\n",
      "+---------+-----+\n",
      "|77.599998| 2012|\n",
      "|81.370003| 2013|\n",
      "|88.089996| 2014|\n",
      "|90.970001| 2015|\n",
      "|75.190002| 2016|\n",
      "+---------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# en SQL \n",
    "spark.sql(\"\"\"SELECT Max(High) as Maximum, SUBSTR(Date, 1, 4) as Annee FROM dtSQL GROUP BY Annee ORDER BY Annee\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|Annee|  Maximum|\n",
      "+-----+---------+\n",
      "| 2012|77.599998|\n",
      "| 2013|81.370003|\n",
      "| 2014|88.089996|\n",
      "| 2015|90.970001|\n",
      "| 2016|75.190002|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# en DSL\n",
    "# nouvelle colonne qui contient l'année\n",
    "dt = dt.withColumn(\"Annee\", F.substring(\"Date\", 1, 4))\n",
    "# par année, maximum de prix correspondant\n",
    "dt.groupby('Annee').agg(F.max('High').alias('Maximum')).orderBy(\"Annee\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q12 : What is the average Close for each Calendar Month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+----+\n",
      "|           Moyenne|Annee|Mois|\n",
      "+------------------+-----+----+\n",
      "|        60.2354999| 2012|  01|\n",
      "|            60.898| 2012|  02|\n",
      "|60.433636818181796| 2012|  03|\n",
      "|60.149000150000006| 2012|  04|\n",
      "|61.456363409090905| 2012|  05|\n",
      "| 67.50380961904762| 2012|  06|\n",
      "| 72.40666661904763| 2012|  07|\n",
      "| 73.04478265217392| 2012|  08|\n",
      "| 74.18157921052631| 2012|  09|\n",
      "| 75.30619061904761| 2012|  10|\n",
      "| 71.10952333333333| 2012|  11|\n",
      "| 69.71100009999999| 2012|  12|\n",
      "| 69.09476142857143| 2013|  01|\n",
      "| 70.62315857894738| 2013|  02|\n",
      "| 73.43649940000002| 2013|  03|\n",
      "| 77.68954572727273| 2013|  04|\n",
      "| 77.81636368181817| 2013|  05|\n",
      "| 74.97800020000001| 2013|  06|\n",
      "| 77.11545418181818| 2013|  07|\n",
      "| 75.22409204545455| 2013|  08|\n",
      "+------------------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### en SQL\n",
    "spark.sql(\"\"\"SELECT Mean(Close) AS Moyenne, SUBSTR(Date, 1, 4) as Annee, SUBSTR(Date, 6, 2) as Mois FROM dtSQL GROUP BY Annee, Mois ORDER BY Annee, Mois\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------------------+\n",
      "|Annee|Mois|           Moyenne|\n",
      "+-----+----+------------------+\n",
      "| 2012|  01|        60.2354999|\n",
      "| 2012|  02|            60.898|\n",
      "| 2012|  03|60.433636818181796|\n",
      "| 2012|  04|60.149000150000006|\n",
      "| 2012|  05|61.456363409090905|\n",
      "| 2012|  06| 67.50380961904762|\n",
      "| 2012|  07| 72.40666661904763|\n",
      "| 2012|  08| 73.04478265217392|\n",
      "| 2012|  09| 74.18157921052631|\n",
      "| 2012|  10| 75.30619061904761|\n",
      "| 2012|  11| 71.10952333333333|\n",
      "| 2012|  12| 69.71100009999999|\n",
      "| 2013|  01| 69.09476142857143|\n",
      "| 2013|  02| 70.62315857894738|\n",
      "| 2013|  03| 73.43649940000002|\n",
      "| 2013|  04| 77.68954572727273|\n",
      "| 2013|  05| 77.81636368181817|\n",
      "| 2013|  06| 74.97800020000001|\n",
      "| 2013|  07| 77.11545418181818|\n",
      "| 2013|  08| 75.22409204545455|\n",
      "+-----+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# en DSL\n",
    "# nouvelle colonne qui contient le mois\n",
    "dt = dt.withColumn(\"Mois\", F.substring(\"Date\", 6, 2))\n",
    "# par année et par mois, moyenne de Close \n",
    "dt.groupby(\"Annee\", \"Mois\").agg(F.mean('Close').alias(\"Moyenne\")).orderBy(\"Annee\", \"Mois\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
